{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vAFNV80-dnCb"
   },
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Machine translation is a challenging task that traditionally involves large statistical models developed using highly sophisticated linguistic knowledge.\n",
    "\n",
    "Neural machine translation is the use of deep neural networks for the problem of machine translation.\n",
    "\n",
    "In this Notebook, you will discover how to develop a neural machine translation system for translating English phrases to Telugu.\n",
    "\n",
    "Steps:\n",
    "\n",
    "* Prepare Data\n",
    "* Preprocess Data\n",
    "* Text to Sequence Conversion\n",
    "* Build Encoder-Decoder Model\n",
    "* Train Model\n",
    "* Translate English to Telugu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UiN_tPYIgx1E"
   },
   "source": [
    "In this Notebook, The [English-Telugu](https://github.com/scionoftech/English_Telugu_Bilingual-Sentence-Pairs) Bilingual-Sentence-Pairs data is used for Neural Machine Translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdzTBuL2adyL"
   },
   "outputs": [],
   "source": [
    "project_path = \"machine_translation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "Rgvg66I4apty",
    "outputId": "e23d679a-3f70-4f29-e7b2-2d02a84ced67"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "import os\n",
    "from numpy import array, argmax, random, take\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, RepeatVector,TimeDistributed\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5MPqERSf-J-"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmIFn4qU198D"
   },
   "outputs": [],
   "source": [
    "# read phrases from english_telugu_data.txt file\n",
    "english_sentances = []\n",
    "telugu_sentances = []\n",
    "with open(project_path+\"english_telugu_data.txt\", mode='rt', encoding='utf-8') as fp:\n",
    "    for line in fp.readlines():\n",
    "        eng_tel = line.split(\"++++$++++\")\n",
    "        english_sentances.append(eng_tel[0])\n",
    "        telugu_sentances.append(eng_tel[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqsyqG-j2nOz"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"english_sentances\":english_sentances,\"telugu_sentances\":telugu_sentances})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "m-Psopq3bHMh",
    "outputId": "746a5d39-ca03-4269-8383-83c76dbf1b7b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentances</th>\n",
       "      <th>telugu_sentances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>His legs are long.</td>\n",
       "      <td>అతని కాళ్ళు పొడవుగా ఉన్నాయి.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who taught Tom how to speak French?</td>\n",
       "      <td>టామ్ ఫ్రెంచ్ మాట్లాడటం ఎలా నేర్పించారు?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I swim in the sea every day.</td>\n",
       "      <td>నేను ప్రతి రోజు సముద్రంలో ఈత కొడతాను.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom popped into the supermarket on his way hom...</td>\n",
       "      <td>టామ్ కొంచెం పాలు కొనడానికి ఇంటికి వెళ్ళేటప్పుడ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smoke filled the room.</td>\n",
       "      <td>పొగ గదిని నింపింది.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   english_sentances                                   telugu_sentances\n",
       "0                                 His legs are long.                     అతని కాళ్ళు పొడవుగా ఉన్నాయి.\\n\n",
       "1                Who taught Tom how to speak French?          టామ్ ఫ్రెంచ్ మాట్లాడటం ఎలా నేర్పించారు?\\n\n",
       "2                       I swim in the sea every day.            నేను ప్రతి రోజు సముద్రంలో ఈత కొడతాను.\\n\n",
       "3  Tom popped into the supermarket on his way hom...  టామ్ కొంచెం పాలు కొనడానికి ఇంటికి వెళ్ళేటప్పుడ...\n",
       "4                             Smoke filled the room.                              పొగ గదిని నింపింది.\\n"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_91YZHuCbI9J",
    "outputId": "86413624-50e1-4dbe-ba9d-30b46d33506e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155798, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUlIOezBKFkO"
   },
   "outputs": [],
   "source": [
    "# Let's take 70000 phrases from data\n",
    "data = data.iloc[:70000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AdriXixpbjuD"
   },
   "source": [
    "## Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtHlGIsPibR5"
   },
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOyqb9FKcjbc"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3xbjiJtbQeK"
   },
   "outputs": [],
   "source": [
    "# clean english sentances\n",
    "def clean_eng(text):\n",
    "    # Lowercase all characters\n",
    "    text = text.lower()\n",
    "    # map contractions\n",
    "    text = ' '.join([contraction_mapping[w] if w in contraction_mapping else w for w in text.split(\" \")])\n",
    "    # Remove quotes\n",
    "    text = re.sub(\"'\", '', text)\n",
    "    # Remove all the special characters\n",
    "    exclude = set(string.punctuation) # Set of all special characters\n",
    "    text = ''.join([c for c in text if c not in exclude])\n",
    "    # Remove all numbers from text\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.translate(remove_digits)\n",
    "    # Remove extra spaces\n",
    "    text= text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fbhk3FuMe-3J"
   },
   "outputs": [],
   "source": [
    "# clean telugu sentances\n",
    "def clean_tel(text):\n",
    "    # Lowercase all characters\n",
    "    text = text.lower()\n",
    "    # Remove quotes\n",
    "    text = re.sub(\"'\", '', text)\n",
    "    # Remove all the special characters\n",
    "    exclude = set(string.punctuation) # Set of all special characters\n",
    "    text = ''.join([c for c in text if c not in exclude])\n",
    "    # Remove all numbers from text\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.translate(remove_digits)\n",
    "    # Remove Telugu numbers from text\n",
    "    text = re.sub(\"[౦౧౨౩౪౫౬౭౮౯]\", '', text)\n",
    "    # Remove extra spaces\n",
    "    text= text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLj0iaLEheFS"
   },
   "outputs": [],
   "source": [
    "# clean text\n",
    "data_df = data.copy()\n",
    "data_df[\"english_sentances\"] = data_df[\"english_sentances\"] .apply(lambda x: clean_eng(x))\n",
    "data_df[\"telugu_sentances\"] = data_df[\"telugu_sentances\"] .apply(lambda x: clean_tel(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "VSujFtrD30j0",
    "outputId": "f336e91a-5462-4a69-a981-2b63b996ae28"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentances</th>\n",
       "      <th>telugu_sentances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>his legs are long</td>\n",
       "      <td>అతని కాళ్ళు పొడవుగా ఉన్నాయి</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who taught tom how to speak french</td>\n",
       "      <td>టామ్ ఫ్రెంచ్ మాట్లాడటం ఎలా నేర్పించారు</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i swim in the sea every day</td>\n",
       "      <td>నేను ప్రతి రోజు సముద్రంలో ఈత కొడతాను</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tom popped into the supermarket on his way hom...</td>\n",
       "      <td>టామ్ కొంచెం పాలు కొనడానికి ఇంటికి వెళ్ళేటప్పుడ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smoke filled the room</td>\n",
       "      <td>పొగ గదిని నింపింది</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   english_sentances                                   telugu_sentances\n",
       "0                                  his legs are long                        అతని కాళ్ళు పొడవుగా ఉన్నాయి\n",
       "1                 who taught tom how to speak french             టామ్ ఫ్రెంచ్ మాట్లాడటం ఎలా నేర్పించారు\n",
       "2                        i swim in the sea every day               నేను ప్రతి రోజు సముద్రంలో ఈత కొడతాను\n",
       "3  tom popped into the supermarket on his way hom...  టామ్ కొంచెం పాలు కొనడానికి ఇంటికి వెళ్ళేటప్పుడ...\n",
       "4                              smoke filled the room                                 పొగ గదిని నింపింది"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQOHFc7LlCYi"
   },
   "source": [
    "#### Text to Sequence Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "qce7tmBPk4Pa",
    "outputId": "c1a77f8c-e94e-4035-e336-279b66b54b2a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndIA1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# empty lists\n",
    "eng_l = []\n",
    "tel_l = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data_df[\"english_sentances\"].values:\n",
    "      eng_l.append(len(i.split()))\n",
    "\n",
    "for i in data_df[\"telugu_sentances\"].values:\n",
    "      tel_l.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'eng':eng_l, 'tel':tel_l})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbCziy1PmCj6"
   },
   "source": [
    "Quite intuitive – the maximum length of the Telugu sentences is 26 and that of the English phrases is 43."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbNcHXYZmdjb"
   },
   "source": [
    "Next, vectorize our text data by using Keras’s Tokenizer() class. It will turn our sentences into sequences of integers. We can then pad those sequences with zeros to make all the sequences of the same length.\n",
    "\n",
    "Note that we will prepare tokenizers for both the Telugu and English sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7m_56NwlNqK"
   },
   "outputs": [],
   "source": [
    "# function to build a tokenizer\n",
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-g_5D6QzlxBZ",
    "outputId": "ad94565a-c7eb-4c5c-f539-0b1710557bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 10315\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = tokenization(data_df[\"english_sentances\"])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "eng_length = 43\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_E5lmeA1lzJR",
    "outputId": "97b84617-d205-40e6-fc1d-85de0c64a67c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu Vocabulary Size: 26680\n"
     ]
    }
   ],
   "source": [
    "# prepare Telugu tokenizer\n",
    "tel_tokenizer = tokenization(data_df[\"telugu_sentances\"])\n",
    "tel_vocab_size = len(tel_tokenizer.word_index) + 1\n",
    "\n",
    "tel_length = 26\n",
    "print('Telugu Vocabulary Size: %d' % tel_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3QvWcFUnRzf"
   },
   "source": [
    "The below code block contains a function to prepare the sequences. It will also perform sequence padding to a maximum sentence length as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVQURgPbnMmn"
   },
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2U75Z67omA4"
   },
   "source": [
    "We will now split the data into train and test set for model training and evaluation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RhvoB18wnbxQ"
   },
   "outputs": [],
   "source": [
    "# split data into train and test set\n",
    "train, test = train_test_split(data_df, test_size=0.2, random_state = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnIOCt8dom0u"
   },
   "source": [
    "It’s time to encode the sentences. We will encode **English sentences as the input sequences** and **Telugu sentences as the target sequences**. This has to be done for both the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DgFTl5FoozS9"
   },
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(eng_tokenizer, eng_length, train[\"english_sentances\"])\n",
    "trainY = encode_sequences(tel_tokenizer, tel_length, train[\"telugu_sentances\"])\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(eng_tokenizer, eng_length, test[\"english_sentances\"])\n",
    "testY = encode_sequences(tel_tokenizer, tel_length, test[\"telugu_sentances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_14q520fp5j2",
    "outputId": "c788e27a-92b4-4c0e-89a1-3fec620497ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56000, 43), (56000, 26), (14000, 43), (14000, 26))"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape,trainY.shape,testX.shape,testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1osUoSnc_F01"
   },
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "# def data_generator(eng_tokenizer,tel_tokenizer, eng_length,tel_length, eng_data,tel_data):\n",
    "#     # loop for each sentance\n",
    "#     while 1:\n",
    "#         for eng_sentance,tel_sentance in zip(eng_data,tel_data):\n",
    "#             # integer encode sequences\n",
    "#             eng_seq = eng_tokenizer.texts_to_sequences(eng_sentance)\n",
    "#             tel_seq = tel_tokenizer.texts_to_sequences(tel_sentance)\n",
    "#             # pad sequences with 0 values\n",
    "#             eng_seq = pad_sequences(eng_seq, maxlen=eng_length, padding='post')\n",
    "#             tel_seq = pad_sequences(tel_seq, maxlen=tel_length, padding='post')\n",
    "#             yield eng_seq,tel_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1PofjbMr-h3"
   },
   "source": [
    "We’ll start off by defining our Seq2Seq model architecture:\n",
    "\n",
    "For the encoder, we will use an embedding layer and an LSTM layer\n",
    "For the decoder, we will use another LSTM layer followed by a dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![encoder_decoder.png](encoder_decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3r-kEDnDpS0B"
   },
   "outputs": [],
   "source": [
    "# build NMT model\n",
    "def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(out_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "uail-QTMpWAA",
    "outputId": "ff860fb4-6291-48ff-8ad6-ef881b4b3dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 43, 512)           5281280   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 26, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 26, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 26, 26680)         13686840  \n",
      "=================================================================\n",
      "Total params: 23,166,520\n",
      "Trainable params: 23,166,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model compilation\n",
    "model = define_model(eng_vocab_size,tel_vocab_size,eng_length,tel_length, 512)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91Fhz_lyr7ro"
   },
   "source": [
    "We are using the RMSprop optimizer in this model as it’s usually a good choice when working with recurrent neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4BrOMpZpXmx"
   },
   "outputs": [],
   "source": [
    "rms = optimizers.RMSprop()\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKgbPZC4ruWQ"
   },
   "source": [
    "Please note that we have used ‘sparse_categorical_crossentropy‘ as the loss function. This is because the function allows us to use the target sequence as is, instead of the one-hot encoded format. **One-hot encoding the target sequences using such a huge vocabulary might consume our system’s entire memory**.\n",
    "\n",
    "We are all set to start training our model!\n",
    "\n",
    "**We will train it for 50 epochs and with a batch size of 128 with a validation split of 20%.** 80% of the data will be used for training the model and the rest for evaluating it. You may change and play around with these hyperparameters.\n",
    "\n",
    "We will also use the **ModelCheckpoint()** function to save the model with the lowest validation loss. I personally prefer this method over early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_pneFkFpfvp"
   },
   "outputs": [],
   "source": [
    "# Defining a helper function to save the model after each epoch \n",
    "# in which the loss decreases \n",
    "filepath = project_path+'NMT_model.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# Defining a helper function to reduce the learning rate each time \n",
    "# the learning plateaus \n",
    "reduce_alpha = ReduceLROnPlateau(monitor ='val_loss', factor = 0.2, patience = 1, min_lr = 0.001)\n",
    "# stop traning if there increase in loss\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n",
    "callbacks = [checkpoint, reduce_alpha] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TyoOKhiFjHM"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "# epochs = 30\n",
    "# train_steps = len(train[\"english_sentances\"])\n",
    "# val_steps = len(test[\"english_sentances\"])\n",
    "# create the data generator\n",
    "# prepare training data\n",
    "#train_gen = data_generator(eng_tokenizer,tel_tokenizer, eng_length,tel_length, train[\"english_sentances\"],train[\"telugu_sentances\"])\n",
    "# prepare validation data\n",
    "#test_gen = data_generator(eng_tokenizer,tel_tokenizer, eng_length,tel_length, test[\"english_sentances\"],test[\"telugu_sentances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcdLnDtTBVSl"
   },
   "outputs": [],
   "source": [
    "# model.fit_generator(train_gen, epochs=epochs, steps_per_epoch=train_steps,validation_data=(test_gen),validation_steps=val_steps,callbacks=callbacks, verbose=1)\n",
    "# save model\n",
    "# model.save(project_path+'model_img_cap_pad.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JhtykhnN5xPz",
    "outputId": "0bf2fb7d-2d03-418d-9dcf-0f6e037efbd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44800 samples, validate on 11200 samples\n",
      "Epoch 1/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 1.4487\n",
      "Epoch 00001: val_loss improved from inf to 1.24224, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 79s 2ms/sample - loss: 1.4480 - val_loss: 1.2422\n",
      "Epoch 2/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 1.1688\n",
      "Epoch 00002: val_loss improved from 1.24224 to 1.13308, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 77s 2ms/sample - loss: 1.1686 - val_loss: 1.1331\n",
      "Epoch 3/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 1.0540\n",
      "Epoch 00003: val_loss improved from 1.13308 to 1.04173, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 77s 2ms/sample - loss: 1.0541 - val_loss: 1.0417\n",
      "Epoch 4/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.9569\n",
      "Epoch 00004: val_loss improved from 1.04173 to 0.98506, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 77s 2ms/sample - loss: 0.9569 - val_loss: 0.9851\n",
      "Epoch 5/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.8731\n",
      "Epoch 00005: val_loss improved from 0.98506 to 0.92367, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 77s 2ms/sample - loss: 0.8731 - val_loss: 0.9237\n",
      "Epoch 6/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.8046\n",
      "Epoch 00006: val_loss improved from 0.92367 to 0.88006, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 77s 2ms/sample - loss: 0.8047 - val_loss: 0.8801\n",
      "Epoch 7/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.7484\n",
      "Epoch 00007: val_loss improved from 0.88006 to 0.85990, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 77s 2ms/sample - loss: 0.7482 - val_loss: 0.8599\n",
      "Epoch 8/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.7006\n",
      "Epoch 00008: val_loss improved from 0.85990 to 0.83252, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 77s 2ms/sample - loss: 0.7007 - val_loss: 0.8325\n",
      "Epoch 9/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.6572\n",
      "Epoch 00009: val_loss did not improve from 0.83252\n",
      "44800/44800 [==============================] - 76s 2ms/sample - loss: 0.6571 - val_loss: 0.8362\n",
      "Epoch 10/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.6190\n",
      "Epoch 00010: val_loss improved from 0.83252 to 0.80703, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 77s 2ms/sample - loss: 0.6193 - val_loss: 0.8070\n",
      "Epoch 11/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.5849\n",
      "Epoch 00011: val_loss improved from 0.80703 to 0.80138, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 77s 2ms/sample - loss: 0.5850 - val_loss: 0.8014\n",
      "Epoch 12/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.5524\n",
      "Epoch 00012: val_loss improved from 0.80138 to 0.80122, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 76s 2ms/sample - loss: 0.5525 - val_loss: 0.8012\n",
      "Epoch 13/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.5225\n",
      "Epoch 00013: val_loss did not improve from 0.80122\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.5225 - val_loss: 0.8027\n",
      "Epoch 14/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.4905\n",
      "Epoch 00014: val_loss improved from 0.80122 to 0.79633, saving model to /content/drive/My Drive/DLCP/openwork/machine_translation/NMT_model.h5\n",
      "44800/44800 [==============================] - 76s 2ms/sample - loss: 0.4905 - val_loss: 0.7963\n",
      "Epoch 15/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.4594\n",
      "Epoch 00015: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 76s 2ms/sample - loss: 0.4596 - val_loss: 0.7975\n",
      "Epoch 16/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.4336\n",
      "Epoch 00016: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.4338 - val_loss: 0.7994\n",
      "Epoch 17/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.4080\n",
      "Epoch 00017: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 76s 2ms/sample - loss: 0.4082 - val_loss: 0.8008\n",
      "Epoch 18/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.3822\n",
      "Epoch 00018: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.3824 - val_loss: 0.8028\n",
      "Epoch 19/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.3609\n",
      "Epoch 00019: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 74s 2ms/sample - loss: 0.3610 - val_loss: 0.8073\n",
      "Epoch 20/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.3407\n",
      "Epoch 00020: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 74s 2ms/sample - loss: 0.3407 - val_loss: 0.8098\n",
      "Epoch 21/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.3182\n",
      "Epoch 00021: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 76s 2ms/sample - loss: 0.3184 - val_loss: 0.8125\n",
      "Epoch 22/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.2984\n",
      "Epoch 00022: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.2984 - val_loss: 0.8149\n",
      "Epoch 23/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.2807\n",
      "Epoch 00023: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.2810 - val_loss: 0.8206\n",
      "Epoch 24/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.2643\n",
      "Epoch 00024: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 74s 2ms/sample - loss: 0.2645 - val_loss: 0.8242\n",
      "Epoch 25/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.2495\n",
      "Epoch 00025: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.2495 - val_loss: 0.8267\n",
      "Epoch 26/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.2346\n",
      "Epoch 00026: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 74s 2ms/sample - loss: 0.2346 - val_loss: 0.8312\n",
      "Epoch 27/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.2208\n",
      "Epoch 00027: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 74s 2ms/sample - loss: 0.2208 - val_loss: 0.8337\n",
      "Epoch 28/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.2089\n",
      "Epoch 00028: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 74s 2ms/sample - loss: 0.2089 - val_loss: 0.8389\n",
      "Epoch 29/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1968\n",
      "Epoch 00029: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.1970 - val_loss: 0.8416\n",
      "Epoch 30/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1852\n",
      "Epoch 00030: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 76s 2ms/sample - loss: 0.1853 - val_loss: 0.8460\n",
      "Epoch 31/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1752\n",
      "Epoch 00031: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.1753 - val_loss: 0.8477\n",
      "Epoch 32/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1651\n",
      "Epoch 00032: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.1651 - val_loss: 0.8575\n",
      "Epoch 33/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1560\n",
      "Epoch 00033: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 74s 2ms/sample - loss: 0.1563 - val_loss: 0.8595\n",
      "Epoch 34/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1479\n",
      "Epoch 00034: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.1480 - val_loss: 0.8635\n",
      "Epoch 35/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1394\n",
      "Epoch 00035: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.1395 - val_loss: 0.8681\n",
      "Epoch 36/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1324\n",
      "Epoch 00036: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.1324 - val_loss: 0.8717\n",
      "Epoch 37/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00037: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 76s 2ms/sample - loss: 0.1256 - val_loss: 0.8751\n",
      "Epoch 38/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1193\n",
      "Epoch 00038: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 77s 2ms/sample - loss: 0.1192 - val_loss: 0.8815\n",
      "Epoch 39/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1130\n",
      "Epoch 00039: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 77s 2ms/sample - loss: 0.1130 - val_loss: 0.8866\n",
      "Epoch 40/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1072\n",
      "Epoch 00040: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 76s 2ms/sample - loss: 0.1072 - val_loss: 0.8894\n",
      "Epoch 41/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.1013\n",
      "Epoch 00041: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 74s 2ms/sample - loss: 0.1013 - val_loss: 0.9000\n",
      "Epoch 42/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.0959\n",
      "Epoch 00042: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 74s 2ms/sample - loss: 0.0960 - val_loss: 0.8979\n",
      "Epoch 43/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.0914\n",
      "Epoch 00043: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.0913 - val_loss: 0.9065\n",
      "Epoch 44/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.0872\n",
      "Epoch 00044: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.0872 - val_loss: 0.9111\n",
      "Epoch 45/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.0834\n",
      "Epoch 00045: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.0834 - val_loss: 0.9178\n",
      "Epoch 46/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.0792\n",
      "Epoch 00046: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 76s 2ms/sample - loss: 0.0793 - val_loss: 0.9184\n",
      "Epoch 47/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.0751\n",
      "Epoch 00047: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.0751 - val_loss: 0.9211\n",
      "Epoch 48/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.0711\n",
      "Epoch 00048: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 76s 2ms/sample - loss: 0.0712 - val_loss: 0.9270\n",
      "Epoch 49/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.0683\n",
      "Epoch 00049: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.0683 - val_loss: 0.9304\n",
      "Epoch 50/50\n",
      "44672/44800 [============================>.] - ETA: 0s - loss: 0.0655\n",
      "Epoch 00050: val_loss did not improve from 0.79633\n",
      "44800/44800 [==============================] - 75s 2ms/sample - loss: 0.0655 - val_loss: 0.9364\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
    "                    epochs=50, batch_size=128, validation_split = 0.2,callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPrYZMXOrpfu"
   },
   "source": [
    "Let’s compare the training loss and the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "EZzRP4nzqPsK",
    "outputId": "c32aec2b-360d-4641-db6f-bf1c9c01105d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P84pEoG7rhk2"
   },
   "source": [
    "Finally, we can load the saved model and make predictions on the unseen data – testX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uDK5VJGXNgT"
   },
   "outputs": [],
   "source": [
    "# get 10 random ids of test samples\n",
    "idx = random.randint(testX.shape[0], size=10)\n",
    "# get 10 encoded english test samples\n",
    "encoded_english_actual = testX[idx,:]\n",
    "# get 10 actual english sentences \n",
    "eng_actual = test[\"english_sentances\"].values\n",
    "eng_actual = eng_actual[idx]\n",
    "# get 10 actual telugu sentences\n",
    "actual = test[\"telugu_sentances\"].values\n",
    "actual = actual[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GFwNVeL4qX9c"
   },
   "outputs": [],
   "source": [
    "# load model weights\n",
    "# model.load_weights(filepath)\n",
    "# predict english sentence to telugu sentence\n",
    "preds = model.predict_classes(encoded_english_actual.reshape((encoded_english_actual.shape[0],encoded_english_actual.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LOZIMzJGrfcG"
   },
   "source": [
    "These predictions are sequences of integers. We need to convert these integers to their corresponding words. Let’s define a function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CnN9MShqe7c"
   },
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXve26ZprcOO"
   },
   "source": [
    "Convert predictions into text (Telugu):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mB9J6bTCqk4k"
   },
   "outputs": [],
   "source": [
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], tel_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], tel_tokenizer)) or (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t) \n",
    "\n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2W6z-8nErZyu"
   },
   "source": [
    "Let’s create a dataframe with original english and telugu sentances and translated telugu sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lt6IjgL9qokM"
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'english_actual':eng_actual, 'telugu_actual' : actual, 'telugu_predicted' : preds_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "3VApyB40qr5k",
    "outputId": "ba3c4297-ec1e-438b-a8f8-0e92463d4c1c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_actual</th>\n",
       "      <th>telugu_actual</th>\n",
       "      <th>telugu_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i like to do that</td>\n",
       "      <td>నేను అలా చేయాలనుకుంటున్నాను</td>\n",
       "      <td>నేను అలా చేయాలనుకుంటున్నాను                   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>she spent some time in boston</td>\n",
       "      <td>ఆమె బోస్టన్‌లో కొంత సమయం గడిపింది</td>\n",
       "      <td>ఆమె నాకు కొంత సమయం</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tom told me to ask everybody</td>\n",
       "      <td>అందరినీ అడగమని టామ్ చెప్పాడు</td>\n",
       "      <td>టామ్ వారిని చేయమని</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i know where you can hide</td>\n",
       "      <td>మీరు ఎక్కడ దాచవచ్చో నాకు తెలుసు</td>\n",
       "      <td>మీరు ఎక్కడ  నాకు తెలుసు</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cookie is under the table</td>\n",
       "      <td>కుకీ టేబుల్ కింద ఉంది</td>\n",
       "      <td>ఎందుకు టేబుల్ ఉంది</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i offered to fix toms flat tire</td>\n",
       "      <td>నేను టామ్ యొక్క ఫ్లాట్ టైర్ను పరిష్కరించడానికి...</td>\n",
       "      <td>టామ్ పదాలు తీసుకోండి విశ్రాంతి ఇచ్చాను        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>everybody was there except for tom</td>\n",
       "      <td>టామ్ తప్ప అందరూ అక్కడ ఉన్నారు</td>\n",
       "      <td>టామ్ తప్ప అందరూ అక్కడ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>these fireworks are spectacular</td>\n",
       "      <td>ఈ బాణసంచా అద్భుతమైనవి</td>\n",
       "      <td>ఈ చివరి వెచ్చగా</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>you have my word</td>\n",
       "      <td>మీకు నా మాట ఉంది</td>\n",
       "      <td>మీకు నా అనుమతి ఉంది</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the committee elected him chairperson</td>\n",
       "      <td>కమిటీ ఆయనను చైర్‌పర్సన్‌గా ఎన్నుకుంది</td>\n",
       "      <td>ఈ అతనికి  ఇవ్వండి చేసింది</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          english_actual  ...                                   telugu_predicted\n",
       "0                      i like to do that  ...  నేను అలా చేయాలనుకుంటున్నాను                   ...\n",
       "1          she spent some time in boston  ...           ఆమె నాకు కొంత సమయం                      \n",
       "2           tom told me to ask everybody  ...          టామ్ వారిని చేయమని                       \n",
       "3              i know where you can hide  ...       మీరు ఎక్కడ  నాకు తెలుసు                     \n",
       "4              cookie is under the table  ...          ఎందుకు టేబుల్ ఉంది                       \n",
       "5        i offered to fix toms flat tire  ...  టామ్ పదాలు తీసుకోండి విశ్రాంతి ఇచ్చాను        ...\n",
       "6     everybody was there except for tom  ...        టామ్ తప్ప అందరూ అక్కడ                      \n",
       "7        these fireworks are spectacular  ...             ఈ చివరి వెచ్చగా                       \n",
       "8                       you have my word  ...          మీకు నా అనుమతి ఉంది                      \n",
       "9  the committee elected him chairperson  ...     ఈ అతనికి  ఇవ్వండి చేసింది                     \n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print 10 rows\n",
    "pred_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MIU-GmMHq3rV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "English_To_Telugu_RepeatVector.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
